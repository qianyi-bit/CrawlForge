使用说明
1. 安装所需的 Python 库
确保你已经安装了以下 Python 库：
pip install requests
2. 准备代理文件
创建一个名为 ip.txt 的文本文件，并将代理以 socks5://<代理地址>:<端口> 的形式逐行写入文件。

示例内容：

socks5://127.0.0.1:9050
socks5://10.0.0.2:1080
socks5://123.45.67.89:9999
3. 准备爬虫参数
headers 和 cookies 已经在脚本中定义，可以根据需要进行调整，尤其是 User-Agent 列表和其他请求头。

base_url: 你可以根据爬取的目标站点修改为实际 API 接口的 URL。

category: 如果要爬取特定类别的内容，可以调整该参数。

page_size 和 max_pages: 调整每页爬取的数据量和最大爬取的页数。

max_workers: 设置最大线程数，以决定并发抓取的效率。

4. 爬取过程
加载代理：爬虫会从 ip.txt 文件中加载 SOCKS5 代理列表，并在每次请求时随机选择一个代理。

多线程爬取：使用 ThreadPoolExecutor 提供并发请求，以加速数据的抓取。

User-Agent 随机化：每次请求都会随机更换 User-Agent，模拟更真实的访问行为。

延迟处理：每次请求前会等待一定的随机时间（0.5 到 1.5 秒），以防止被服务器检测为爬虫。

5. 运行代码
运行代码时，只需直接执行 Python 文件：
python your_script_name.py
代码会根据配置开始从目标站点抓取数据，直到抓取到指定的页数或者到达最后一页。

6. 输出结果
所有爬取的数据会保存到 african_business.csv 文件中，文件格式如下：

url	time	title	content	image
...	...	...	...	...

7. 如何调整设置
代理列表：将更多的 SOCKS5 代理添加到 ip.txt 文件中，增加代理池的有效性。

页面爬取数量：修改 max_pages 来决定爬取多少页，page_size 用于控制每页抓取的最大数据量。

线程数：可以通过调整 max_workers 来改变并发请求的数量，合理选择线程数以防止过高的并发导致目标服务器封禁。

8. 注意事项
合法性：请确保你遵守目标站点的 robots.txt 规定，不要进行恶意爬取。遵循站点的访问频率要求，避免造成过大的负载。

代理稳定性：如果代理不稳定，可以使用更稳定的代理提供商，或者适当增加重试机制。

抓取速度与频率：控制爬取的速度，避免请求过于频繁造成 IP 被封。可以通过增加延迟来避免这一问题。

9. 可能遇到的问题
代理问题：如果代理池中的代理不可用，爬虫会打印相应错误信息，并跳过该页数据。

请求超时或错误：代码中已经处理了超时和请求错误，爬虫会跳过出错的页面并继续爬取其他页面。

防止被封：通过更换 User-Agent 和使用代理池来尽量降低被目标站点封锁的风险。

通过以上步骤，你可以顺利使用该爬虫脚本来抓取数据并保存为 CSV 文件。如果有任何问题或需要进一步定制功能，可以根据具体需求对代码进行修改。
